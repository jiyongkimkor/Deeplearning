{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43308f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from os.path import isfile\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import transforms\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c8add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#전역변수 설정\n",
    "\n",
    "url = \"https://www.cs.toronto.edu/~vmnih/data/\"\n",
    "\n",
    "train_input_url = \"mass_roads/train/sat/index.html\"\n",
    "train_target_url = \"mass_roads/train/map/index.html\"\n",
    "\n",
    "train_directory = \"C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2. Road extraction/datasets/train/input\"\n",
    "target_directory = \"C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2. Road extraction/datasets/train/target\"\n",
    "\n",
    "dir_train = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2. Road extraction/datasets/train/input_filtered'\n",
    "dir_target = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2. Road extraction/datasets/train/target_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261c3360",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DataSet 설정 및 Trasform 과정\n",
    "#train 파일(R,G,B)을 img1에 저장 mask(binary)를 img2에 저장 함\n",
    "#두 파일을 합쳐서 4차원으로 stacking 하고 img3로 지정, normalization 및 crop 진행\n",
    "#crop 진행 후, img3를 다시 분할해서 img1과 img2로 나눔\n",
    "\n",
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, train_paths, target_paths, transform=None): \n",
    "        self.train_paths = [f for f in os.listdir(dir_train) if isfile(dir_train + '/'+ f)] \n",
    "        self.target_paths = [f for f in os.listdir(dir_target) if isfile(dir_target + '/' + f)] \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index): \n",
    "        train = np.array(Image.open(dir_train+'/'+self.train_paths[index]))\n",
    "        target = np.array(Image.open(dir_target+'/'+self.target_paths[index]))\n",
    "        \n",
    "        if self.transform:\n",
    "            #transforms.Compose\n",
    "            data = self.transform(image=train, mask=target)\n",
    "            data_img = data[\"image\"]\n",
    "            data_lab = data[\"mask\"]\n",
    "            \n",
    "            data = {'train': data_img, 'target': data_lab}\n",
    "        return data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_paths)\n",
    "    \n",
    "trans = A.Compose([\n",
    "    A.HorizontalFlip(),\n",
    "    A.VerticalFlip(),\n",
    "    A.Normalize(mean=0.5, std=0.5),\n",
    "    transforms.ToTensorV2(transpose_mask=True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b632de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unet 모델 쌓기\n",
    "\n",
    "class UNet(nn.Module):  \n",
    "    def __init__(self): \n",
    "# super(subclass, self) : subclass에서 base class의 내용을 오버라이드해서 사용하고 싶을 때\n",
    "        super(UNet, self).__init__() \n",
    "        \n",
    "# 네트워크에서 반복적으로 사용되는 Conv + BatchNorm + Relu를 합쳐서 하나의 함수로 정의\n",
    "       \tdef CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers) # *으로 list unpacking \n",
    "\n",
    "            return cbr\n",
    "\n",
    "        # Contracting path\n",
    "        self.enc1_1 = CBR2d(in_channels=3, out_channels=64)\n",
    "        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n",
    "        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n",
    "        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n",
    "        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n",
    "        # Expansive path\n",
    "        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n",
    "\n",
    "        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec4_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n",
    "        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n",
    "        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec3_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n",
    "        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n",
    "        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec2_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n",
    "        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n",
    "        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec1_2 = CBR2d(in_channels=2 * 64, out_channels=64)\n",
    "        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n",
    "        self.fc = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=1)\n",
    "        \n",
    "# __init__ 함수에서 선언한 layer들 연결해서 data propa flow 만들기\n",
    "    def forward(self, x):\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "        \n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "        \n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "        \n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "        \n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        dec5_1 = self.dec5_1(enc5_1)\n",
    "        \n",
    "        unpool4 = self.unpool4(dec5_1)\n",
    "        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n",
    "        dec4_2 = self.dec4_2(cat4)\n",
    "        dec4_1 = self.dec4_1(dec4_2)\n",
    "        \n",
    "        unpool3 = self.unpool3(dec4_1)\n",
    "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
    "        dec3_2 = self.dec3_2(cat3)\n",
    "        dec3_1 = self.dec3_1(dec3_2)\n",
    "\n",
    "        unpool2 = self.unpool2(dec3_1)\n",
    "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
    "        dec2_2 = self.dec2_2(cat2)\n",
    "        dec2_1 = self.dec2_1(dec2_2)\n",
    "\n",
    "        unpool1 = self.unpool1(dec2_1)\n",
    "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
    "        #tensor 합치기: Unet에서 Copy and Crop 과정을 담당한다\n",
    "        dec1_2 = self.dec1_2(cat1)\n",
    "        dec1_1 = self.dec1_1(dec1_2)\n",
    "\n",
    "        out = self.fc(dec1_1)\n",
    "\n",
    "        return out # data가 모든 layer를 거쳐서 나온 output 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "808fbb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1373,  0.1765,  0.0353,  ...,  0.2706,  0.2706, -0.2314],\n",
      "         [-0.1216,  0.0039, -0.0667,  ...,  0.0980, -0.0588, -0.4667],\n",
      "         [-0.0118, -0.0431,  0.0588,  ...,  0.2235,  0.1451, -0.2314],\n",
      "         ...,\n",
      "         [-0.5922, -0.5765, -0.6000,  ..., -0.3020, -0.3098, -0.3725],\n",
      "         [-0.6078, -0.6000, -0.6078,  ..., -0.1608, -0.2314, -0.1059],\n",
      "         [-0.6392, -0.6392, -0.6392,  ..., -0.1529, -0.2314,  0.0118]],\n",
      "\n",
      "        [[-0.3176, -0.0039, -0.1137,  ...,  0.0824,  0.0824, -0.4431],\n",
      "         [-0.3020, -0.1686, -0.2157,  ..., -0.0902, -0.2471, -0.6784],\n",
      "         [-0.1843, -0.2157, -0.0902,  ...,  0.0353, -0.0431, -0.4431],\n",
      "         ...,\n",
      "         [-0.4275, -0.4353, -0.4353,  ..., -0.3647, -0.3647, -0.4275],\n",
      "         [-0.4667, -0.4667, -0.4667,  ..., -0.2471, -0.3176, -0.1686],\n",
      "         [-0.5059, -0.5059, -0.5059,  ..., -0.2549, -0.3176, -0.0745]],\n",
      "\n",
      "        [[-0.4431, -0.1294, -0.2314,  ...,  0.0039, -0.0118, -0.5294],\n",
      "         [-0.4275, -0.2784, -0.3333,  ..., -0.1686, -0.3412, -0.7647],\n",
      "         [-0.2941, -0.3255, -0.2000,  ..., -0.0431, -0.1373, -0.5294],\n",
      "         ...,\n",
      "         [-0.5608, -0.5608, -0.5686,  ..., -0.5137, -0.5137, -0.5765],\n",
      "         [-0.5922, -0.5922, -0.5922,  ..., -0.4039, -0.4745, -0.3333],\n",
      "         [-0.6314, -0.6314, -0.6314,  ..., -0.4039, -0.4902, -0.2471]]])\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39 216 217 218 219 220 221 222 223 224 225 226 227 228 229\n",
      " 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247\n",
      " 248 249 250 251 252 253 254 255]\n"
     ]
    }
   ],
   "source": [
    "#datloader 설정\n",
    "#train은 images1에 mask는 images2에 저장\n",
    "dataset = CustomDataset(dir_train, dir_target, transform=trans)\n",
    "data = dataset.__getitem__(0)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "batch_iterator = iter(train_dataloader)\n",
    "images1, images2 = next(batch_iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7179399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#트레이닝\n",
    "## 하이퍼 파라미터 설정\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epoch = 100\n",
    "\n",
    "data_dir = '/content/drive/My Drive/Colab Notebooks/파이토치/Architecture practice/UNet/data'\n",
    "ckpt_dir = '/content/drive/My Drive/Colab Notebooks/파이토치/Architecture practice/UNet/checkpoint'\n",
    "log_dir = '/content/drive/My Drive/Colab Notebooks/파이토치/Architecture practice/UNet/log'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# transform 적용해서 데이터 셋 불러오기\n",
    "dataset = CustomDataset(dir_train, dir_target, transform=trans)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# val set도 동일하게 진행\n",
    "dataset_val = Dataset(data_dir=os.path.join(data_dir,'val'),transform = transform)\n",
    "loader_val = DataLoader(dataset_val, batch_size=batch_size , shuffle=True)\n",
    "\n",
    "# 네트워크 불러오기\n",
    "net = UNet().to(device) # device : cpu or gpu\n",
    "\n",
    "# loss 정의\n",
    "fn_loss = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "# Optimizer 정의\n",
    "optim = torch.optim.Adam(net.parameters(), lr = lr ) \n",
    "\n",
    "# 기타 variables 설정\n",
    "num_train = len(dataset_train)\n",
    "num_val = len(dataset_val)\n",
    "\n",
    "num_train_for_epoch = np.ceil(num_train/batch_size) # np.ceil : 소수점 반올림\n",
    "num_val_for_epoch = np.ceil(num_val/batch_size)\n",
    "\n",
    "# 기타 function 설정\n",
    "fn_tonumpy = lambda x : x.to('cpu').detach().numpy().transpose(0,2,3,1) # device 위에 올라간 텐서를 detach 한 뒤 numpy로 변환\n",
    "fn_denorm = lambda x, mean, std : (x * std) + mean \n",
    "fn_classifier = lambda x :  1.0 * (x > 0.5)  # threshold 0.5 기준으로 indicator function으로 classifier 구현\n",
    "\n",
    "# Tensorbord\n",
    "writer_train = SummaryWriter(log_dir=os.path.join(log_dir,'train'))\n",
    "writer_val = SummaryWriter(log_dir = os.path.join(log_dir,'val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcff3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네트워크 저장하기\n",
    "# train을 마친 네트워크 저장 \n",
    "# net : 네트워크 파라미터, optim  두개를 dict 형태로 저장\n",
    "def save(ckpt_dir,net,optim,epoch):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    torch.save({'net':net.state_dict(),'optim':optim.state_dict()},'%s/model_epoch%d.pth'%(ckpt_dir,epoch))\n",
    "\n",
    "# 네트워크 불러오기\n",
    "def load(ckpt_dir,net,optim):\n",
    "    if not os.path.exists(ckpt_dir): # 저장된 네트워크가 없다면 인풋을 그대로 반환\n",
    "        epoch = 0\n",
    "        return net, optim, epoch\n",
    "    \n",
    "    ckpt_lst = os.listdir(ckpt_dir) # ckpt_dir 아래 있는 모든 파일 리스트를 받아온다\n",
    "    ckpt_lst.sort(key = lambda f : int(''.join(filter(str,isdigit,f))))\n",
    "\n",
    "    dict_model = torch.load('%s/%s' % (ckpt_dir,ckpt_lst[-1]))\n",
    "\n",
    "    net.load_state_dict(dict_model['net'])\n",
    "    optim.load_state_dict(dict_model['optim'])\n",
    "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
    "\n",
    "    return net,optim,epoch\n",
    "\n",
    "\n",
    "# 네트워크 학습시키기\n",
    "start_epoch = 0\n",
    "net, optim, start_epoch = load(ckpt_dir = ckpt_dir, net = net, optim = optim) # 저장된 네트워크 불러오기\n",
    "\n",
    "for epoch in range(start_epoch+1,num_epoch +1):\n",
    "    net.train()\n",
    "    loss_arr = []\n",
    "\n",
    "    for batch, data in enumerate(loader_train,1): # 1은 뭐니 > index start point\n",
    "        # forward\n",
    "        label = data['label'].to(device)   # 데이터 device로 올리기     \n",
    "        inputs = data['input'].to(device)\n",
    "        output = net(inputs) \n",
    "\n",
    "        # backward\n",
    "        optim.zero_grad()  # gradient 초기화\n",
    "        loss = fn_loss(output, label)  # output과 label 사이의 loss 계산\n",
    "        loss.backward() # gradient backpropagation\n",
    "        optim.step() # backpropa 된 gradient를 이용해서 각 layer의 parameters update\n",
    "\n",
    "        # save loss\n",
    "        loss_arr += [loss.item()]\n",
    "\n",
    "        # tensorbord에 결과값들 저정하기\n",
    "        label = fn_tonumpy(label)\n",
    "        inputs = fn_tonumpy(fn_denorm(inputs,0.5,0.5))\n",
    "        output = fn_tonumpy(fn_classifier(output))\n",
    "\n",
    "        writer_train.add_image('label', label, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_train.add_image('input', inputs, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_train.add_image('output', output, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "\n",
    "    writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n",
    "\n",
    "    \n",
    "    # validation\n",
    "    with torch.no_grad(): # validation 이기 때문에 backpropa 진행 x, 학습된 네트워크가 정답과 얼마나 가까운지 loss만 계산\n",
    "        net.eval() # 네트워크를 evaluation 용으로 선언\n",
    "        loss_arr = []\n",
    "\n",
    "        for batch, data in enumerate(loader_val,1):\n",
    "            # forward\n",
    "            label = data['label'].to(device)\n",
    "            inputs = data['input'].to(device)\n",
    "            output = net(inputs)\n",
    "\n",
    "            # loss \n",
    "            loss = fn_loss(output,label)\n",
    "            loss_arr += [loss.item()]\n",
    "            print('valid : epoch %04d / %04d | Batch %04d \\ %04d | Loss %04d'%(epoch,num_epoch,batch,num_val_for_epoch,np.mean(loss_arr)))\n",
    "\n",
    "            # Tensorboard 저장하기\n",
    "            label = fn_tonumpy(label)\n",
    "            inputs = fn_tonumpy(fn_denorm(inputs, mean=0.5, std=0.5))\n",
    "            output = fn_tonumpy(fn_classifier(output))\n",
    "\n",
    "            writer_val.add_image('label', label, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            writer_val.add_image('input', inputs, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            writer_val.add_image('output', output, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "\n",
    "        writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n",
    "\n",
    "        # epoch이 끝날때 마다 네트워크 저장\n",
    "        save(ckpt_dir=ckpt_dir, net = net, optim = optim, epoch = epoch)\n",
    "\n",
    "writer_train.close()\n",
    "writer_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8098a87",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 1500, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]], dtype=uint8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#이미지 stacking\n",
    "a = os.listdir(dir_train)\n",
    "img1 = np.array(Image.open(dir_train+'/'+a[0]))\n",
    "img2 = np.array(Image.open(dir_target+'/'+a[0]))\n",
    "\n",
    "img3 = np.dstack([img1,img2])\n",
    "print(img3[:,:,0:3].shape)\n",
    "img3[:,:,3:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b412293",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'albumentations.pytorch.transforms' has no attribute 'ToPILImage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12732/2783205560.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Tensor을 Image로 출력하는 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mimg_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimg_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'albumentations.pytorch.transforms' has no attribute 'ToPILImage'"
     ]
    }
   ],
   "source": [
    "#Tensor을 Image로 출력하는 함수\n",
    "tf = transforms.ToPILImage()\n",
    "img_t = tf(dataset.__getitem__(0)[1])\n",
    "img_t.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eed764ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor을 Image로 출력하는 함수\n",
    "tf = transforms.ToPILImage()\n",
    "img_t = tf(images1[0])\n",
    "img_t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b34c443a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]]])\n"
     ]
    }
   ],
   "source": [
    "print(images2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30f6b871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.99215686 -0.9843137  -0.9764706  -0.96862745 -0.9607843\n",
      " -0.9529412  -0.94509804 -0.9372549  -0.92941177 -0.92156863 -0.9137255\n",
      " -0.90588236 -0.8980392  -0.8901961  -0.88235295 -0.8745098  -0.8666667\n",
      " -0.85882354 -0.8509804  -0.84313726 -0.8352941  -0.827451   -0.81960785\n",
      " -0.8117647  -0.8039216  -0.7882353  -0.78039217 -0.77254903 -0.7647059\n",
      " -0.75686276 -0.7490196  -0.7411765  -0.7254902  -0.7176471  -0.70980394\n",
      "  0.7019608   0.7176471   0.7254902   0.7411765   0.7490196   0.75686276\n",
      "  0.7647059   0.77254903  0.78039217  0.7882353   0.79607844  0.8039216\n",
      "  0.8117647   0.81960785  0.827451    0.8352941   0.84313726  0.8509804\n",
      "  0.85882354  0.8666667   0.8745098   0.88235295  0.8901961   0.8980392\n",
      "  0.90588236  0.9137255   0.92156863  0.92941177  0.9372549   0.94509804\n",
      "  0.9529412   0.9607843   0.96862745  0.9764706   0.9843137   0.99215686\n",
      "  1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(dataset.__getitem__(0)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d6a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "505.382px",
    "left": "1484.25px",
    "right": "20px",
    "top": "233.931px",
    "width": "531.285px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
