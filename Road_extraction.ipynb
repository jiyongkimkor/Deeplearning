{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43308f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from os.path import isfile\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import transforms\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c8add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#전역변수 설정\n",
    "\n",
    "url = \"https://www.cs.toronto.edu/~vmnih/data/\"\n",
    "\n",
    "train_input_url = \"mass_roads/train/sat/index.html\"\n",
    "train_target_url = \"mass_roads/train/map/index.html\"\n",
    "val_input_url = \"mass_roads/valid/sat/index.html\"\n",
    "val_target_url = \"mass_roads/valid/map/index.html\"\n",
    "\n",
    "train_directory = \"C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/train/input\"\n",
    "target_directory = \"C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/train/target\"\n",
    "val_train_directory = \"C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/validation/input\"\n",
    "val_target_directory = \"C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/validation/target\"\n",
    "\n",
    "dir_train = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/train/input_filtered'\n",
    "dir_target = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/train/target_filtered'\n",
    "val_train = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/validation/input_filtered'\n",
    "val_target = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/validation/target_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261c3360",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DataSet 설정 및 Trasform 과정\n",
    "#train 파일(R,G,B)을 img1에 저장 mask(binary)를 img2에 저장 함\n",
    "#두 파일을 합쳐서 4차원으로 stacking 하고 img3로 지정, normalization 및 crop 진행\n",
    "#crop 진행 후, img3를 다시 분할해서 img1과 img2로 나눔\n",
    "\n",
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, train_paths, target_paths, transform=None): \n",
    "        self.train_paths = [f for f in os.listdir(dir_train) if isfile(dir_train + '/'+ f)] \n",
    "        self.target_paths = [f for f in os.listdir(dir_target) if isfile(dir_target + '/' + f)] \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index): \n",
    "        train = np.array(Image.open(dir_train+'/'+self.train_paths[index]))\n",
    "        target = np.array(Image.open(dir_target+'/'+self.target_paths[index]))\n",
    "        \n",
    "        if self.transform:\n",
    "            #transforms.Compose\n",
    "            data = self.transform(image=train, mask=target)\n",
    "            data_img = data[\"image\"]\n",
    "            data_lab = data[\"mask\"]\n",
    "            \n",
    "            data = {'train': data_img, 'target': data_lab}\n",
    "        return data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_paths)\n",
    "    \n",
    "trans = A.Compose([\n",
    "    A.HorizontalFlip(),\n",
    "    A.VerticalFlip(),\n",
    "    A.Normalize(mean=0.5, std=0.5),\n",
    "    transforms.ToTensorV2(transpose_mask=True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b632de",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Unet 모델 쌓기\n",
    "\n",
    "class UNet(nn.Module):  \n",
    "    def __init__(self): \n",
    "# super(subclass, self) : subclass에서 base class의 내용을 오버라이드해서 사용하고 싶을 때\n",
    "        super(UNet, self).__init__() \n",
    "        \n",
    "# 네트워크에서 반복적으로 사용되는 Conv + BatchNorm + Relu를 합쳐서 하나의 함수로 정의\n",
    "       \tdef CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers) # *으로 list unpacking \n",
    "\n",
    "            return cbr\n",
    "\n",
    "        # Contracting path\n",
    "        self.enc1_1 = CBR2d(in_channels=3, out_channels=64)\n",
    "        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n",
    "        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n",
    "        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n",
    "        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n",
    "        # Expansive path\n",
    "        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n",
    "\n",
    "        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec4_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n",
    "        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n",
    "        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec3_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n",
    "        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n",
    "        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec2_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n",
    "        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n",
    "        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec1_2 = CBR2d(in_channels=2 * 64, out_channels=64)\n",
    "        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n",
    "        self.fc = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        \n",
    "# __init__ 함수에서 선언한 layer들 연결해서 data propa flow 만들기\n",
    "    def forward(self, x):\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "        \n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "        \n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "        \n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "        \n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        dec5_1 = self.dec5_1(enc5_1)\n",
    "        \n",
    "        unpool4 = self.unpool4(dec5_1)\n",
    "        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n",
    "        dec4_2 = self.dec4_2(cat4)\n",
    "        dec4_1 = self.dec4_1(dec4_2)\n",
    "        \n",
    "        unpool3 = self.unpool3(dec4_1)\n",
    "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
    "        dec3_2 = self.dec3_2(cat3)\n",
    "        dec3_1 = self.dec3_1(dec3_2)\n",
    "\n",
    "        unpool2 = self.unpool2(dec3_1)\n",
    "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
    "        dec2_2 = self.dec2_2(cat2)\n",
    "        dec2_1 = self.dec2_1(dec2_2)\n",
    "\n",
    "        unpool1 = self.unpool1(dec2_1)\n",
    "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
    "        #tensor 합치기: Unet에서 Copy and Crop 과정을 담당한다\n",
    "        dec1_2 = self.dec1_2(cat1)\n",
    "        dec1_1 = self.dec1_1(dec1_2)\n",
    "\n",
    "        out = self.fc(dec1_1)\n",
    "\n",
    "        return out # data가 모든 layer를 거쳐서 나온 output 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7179399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#트레이닝\n",
    "## 하이퍼 파라미터 설정\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 1\n",
    "num_epoch = 100\n",
    "\n",
    "ckpt_dir = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/checkpntlog/checkpoint'\n",
    "log_dir = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/checkpntlog/log'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# transform 적용해서 데이터 셋 불러오기\n",
    "dataset_train = CustomDataset(dir_train, dir_target, transform=trans)\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# val set도 동일하게 진행\n",
    "dataset_val = CustomDataset(val_train, val_target,transform = trans)\n",
    "loader_val = DataLoader(dataset_val, batch_size=batch_size , shuffle=True)\n",
    "\n",
    "# 네트워크 불러오기\n",
    "net = UNet().to(device) # device : cpu or gpu\n",
    "\n",
    "# loss 정의\n",
    "fn_loss = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "# Optimizer 정의\n",
    "optim = torch.optim.Adam(net.parameters(), lr = lr ) \n",
    "\n",
    "# 기타 variables 설정\n",
    "num_train = len(dataset_train)\n",
    "num_val = len(dataset_val)\n",
    "\n",
    "num_train_for_epoch = np.ceil(num_train/batch_size) # np.ceil : 소수점 반올림\n",
    "num_val_for_epoch = np.ceil(num_val/batch_size)\n",
    "\n",
    "# 기타 function 설정\n",
    "fn_tonumpy = lambda x : x.to('cpu').detach().numpy().transpose(0,2,3,1) # device 위에 올라간 텐서를 detach 한 뒤 numpy로 변환\n",
    "fn_denorm = lambda x, mean, std : (x * std) + mean \n",
    "fn_classifier = lambda x :  1.0 * (x > 0.5)  # threshold 0.5 기준으로 indicator function으로 classifier 구현\n",
    "\n",
    "# Tensorbord\n",
    "writer_train = SummaryWriter(log_dir=os.path.join(log_dir,'train'))\n",
    "writer_val = SummaryWriter(log_dir = os.path.join(log_dir,'val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dcff3af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 186 but got size 187 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7348/3462363103.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 데이터 device로 올리기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7348/1841503705.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0munpool4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpool4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec5_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mcat4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munpool4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc4_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mdec4_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdec4_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mdec4_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdec4_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec4_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 186 but got size 187 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# 네트워크 저장하기\n",
    "# train을 마친 네트워크 저장 \n",
    "# net : 네트워크 파라미터, optim  두개를 dict 형태로 저장\n",
    "\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def save(ckpt_dir,net,optim,epoch):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    torch.save({'net':net.state_dict(),'optim':optim.state_dict()},'%s/model_epoch%d.pth'%(ckpt_dir,epoch))\n",
    "\n",
    "# 네트워크 불러오기\n",
    "def load(ckpt_dir,net,optim):\n",
    "    if not os.path.exists(ckpt_dir): # 저장된 네트워크가 없다면 인풋을 그대로 반환\n",
    "        epoch = 0\n",
    "        return net, optim, epoch\n",
    "    \n",
    "    ckpt_lst = os.listdir(ckpt_dir) # ckpt_dir 아래 있는 모든 파일 리스트를 받아온다\n",
    "    ckpt_lst.sort(key = lambda f : int(''.join(filter(str,isdigit,f))))\n",
    "\n",
    "    dict_model = torch.load('%s/%s' % (ckpt_dir,ckpt_lst[-1]))\n",
    "\n",
    "    net.load_state_dict(dict_model['net'])\n",
    "    optim.load_state_dict(dict_model['optim'])\n",
    "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
    "\n",
    "    return net,optim,epoch\n",
    "\n",
    "\n",
    "# 네트워크 학습시키기\n",
    "start_epoch = 0\n",
    "net, optim, start_epoch = load(ckpt_dir = ckpt_dir, net = net, optim = optim) # 저장된 네트워크 불러오기\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(start_epoch+1,num_epoch +1):\n",
    "    net.train()\n",
    "    loss_arr = []\n",
    "    for batch, data in enumerate(loader_train,1): # 1은 뭐니 > index start point\n",
    "        # forward\n",
    "        label = data['target'].to(device)   # 데이터 device로 올리기     \n",
    "        inputs = data['train'].to(device)\n",
    "        output = net(inputs) \n",
    "\n",
    "        # backward\n",
    "        optim.zero_grad()  # gradient 초기화\n",
    "        loss = fn_loss(output, label)  # output과 label 사이의 loss 계산\n",
    "        loss.backward() # gradient backpropagation\n",
    "        optim.step() # backpropa 된 gradient를 이용해서 각 layer의 parameters update\n",
    "\n",
    "        # save loss\n",
    "        loss_arr += [loss.item()]\n",
    "\n",
    "        # tensorbord에 결과값들 저정하기\n",
    "        label = fn_tonumpy(label)\n",
    "        inputs = fn_tonumpy(fn_denorm(inputs,0.5,0.5))\n",
    "        output = fn_tonumpy(fn_classifier(output))\n",
    "\n",
    "        writer_train.add_image('label', label, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_train.add_image('input', inputs, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_train.add_image('output', output, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "\n",
    "    writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n",
    "\n",
    "    \n",
    "    # validation\n",
    "    with torch.no_grad(): # validation 이기 때문에 backpropa 진행 x, 학습된 네트워크가 정답과 얼마나 가까운지 loss만 계산\n",
    "        net.eval() # 네트워크를 evaluation 용으로 선언\n",
    "        loss_arr = []\n",
    "\n",
    "        for batch, data in enumerate(loader_val,1):\n",
    "            # forward\n",
    "            label = data['label'].to(device)\n",
    "            inputs = data['input'].to(device)\n",
    "            output = net(inputs)\n",
    "\n",
    "            # loss \n",
    "            loss = fn_loss(output,label)\n",
    "            loss_arr += [loss.item()]\n",
    "            print('valid : epoch %04d / %04d | Batch %04d \\ %04d | Loss %04d'%(epoch,num_epoch,batch,num_val_for_epoch,np.mean(loss_arr)))\n",
    "\n",
    "            # Tensorboard 저장하기\n",
    "            label = fn_tonumpy(label)\n",
    "            inputs = fn_tonumpy(fn_denorm(inputs, mean=0.5, std=0.5))\n",
    "            output = fn_tonumpy(fn_classifier(output))\n",
    "\n",
    "            writer_val.add_image('label', label, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            writer_val.add_image('input', inputs, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            writer_val.add_image('output', output, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "\n",
    "        writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n",
    "\n",
    "        # epoch이 끝날때 마다 네트워크 저장\n",
    "        save(ckpt_dir=ckpt_dir, net = net, optim = optim, epoch = epoch)\n",
    "\n",
    "writer_train.close()\n",
    "writer_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8098a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 1500, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]], dtype=uint8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#이미지 stacking\n",
    "a = os.listdir(dir_train)\n",
    "img1 = np.array(Image.open(dir_train+'/'+a[0]))\n",
    "img2 = np.array(Image.open(dir_target+'/'+a[0]))\n",
    "\n",
    "img3 = np.dstack([img1,img2])\n",
    "print(img3[:,:,0:3].shape)\n",
    "img3[:,:,3:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b412293",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'albumentations.pytorch.transforms' has no attribute 'ToPILImage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12732/2783205560.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Tensor을 Image로 출력하는 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mimg_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimg_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'albumentations.pytorch.transforms' has no attribute 'ToPILImage'"
     ]
    }
   ],
   "source": [
    "#Tensor을 Image로 출력하는 함수\n",
    "tf = transforms.ToPILImage()\n",
    "img_t = tf(dataset.__getitem__(0)[1])\n",
    "img_t.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eed764ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor을 Image로 출력하는 함수\n",
    "tf = transforms.ToPILImage()\n",
    "img_t = tf(images1[0])\n",
    "img_t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b34c443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]]])\n"
     ]
    }
   ],
   "source": [
    "print(images2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30f6b871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 255]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(dataset.__getitem__(0)['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed2d6a9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "808fbb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datloader 설정\n",
    "#train은 images1에 mask는 images2에 저장\n",
    "dataset = CustomDataset(dir_train, dir_target, transform=trans)\n",
    "data = dataset.__getitem__(0)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "batch_iterator = iter(train_dataloader)\n",
    "images1, images2 = next(batch_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f5cd65a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "              ReLU-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4         [-1, 64, 256, 256]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 256, 256]             128\n",
      "              ReLU-6         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-7         [-1, 64, 128, 128]               0\n",
      "            Conv2d-8        [-1, 128, 128, 128]          73,856\n",
      "       BatchNorm2d-9        [-1, 128, 128, 128]             256\n",
      "             ReLU-10        [-1, 128, 128, 128]               0\n",
      "           Conv2d-11        [-1, 128, 128, 128]         147,584\n",
      "      BatchNorm2d-12        [-1, 128, 128, 128]             256\n",
      "             ReLU-13        [-1, 128, 128, 128]               0\n",
      "        MaxPool2d-14          [-1, 128, 64, 64]               0\n",
      "           Conv2d-15          [-1, 256, 64, 64]         295,168\n",
      "      BatchNorm2d-16          [-1, 256, 64, 64]             512\n",
      "             ReLU-17          [-1, 256, 64, 64]               0\n",
      "           Conv2d-18          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-19          [-1, 256, 64, 64]             512\n",
      "             ReLU-20          [-1, 256, 64, 64]               0\n",
      "        MaxPool2d-21          [-1, 256, 32, 32]               0\n",
      "           Conv2d-22          [-1, 512, 32, 32]       1,180,160\n",
      "      BatchNorm2d-23          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-24          [-1, 512, 32, 32]               0\n",
      "           Conv2d-25          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-26          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-27          [-1, 512, 32, 32]               0\n",
      "        MaxPool2d-28          [-1, 512, 16, 16]               0\n",
      "           Conv2d-29         [-1, 1024, 16, 16]       4,719,616\n",
      "      BatchNorm2d-30         [-1, 1024, 16, 16]           2,048\n",
      "             ReLU-31         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-32          [-1, 512, 16, 16]       4,719,104\n",
      "      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-34          [-1, 512, 16, 16]               0\n",
      "  ConvTranspose2d-35          [-1, 512, 32, 32]       1,049,088\n",
      "           Conv2d-36          [-1, 512, 32, 32]       4,719,104\n",
      "      BatchNorm2d-37          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-38          [-1, 512, 32, 32]               0\n",
      "           Conv2d-39          [-1, 256, 32, 32]       1,179,904\n",
      "      BatchNorm2d-40          [-1, 256, 32, 32]             512\n",
      "             ReLU-41          [-1, 256, 32, 32]               0\n",
      "  ConvTranspose2d-42          [-1, 256, 64, 64]         262,400\n",
      "           Conv2d-43          [-1, 256, 64, 64]       1,179,904\n",
      "      BatchNorm2d-44          [-1, 256, 64, 64]             512\n",
      "             ReLU-45          [-1, 256, 64, 64]               0\n",
      "           Conv2d-46          [-1, 128, 64, 64]         295,040\n",
      "      BatchNorm2d-47          [-1, 128, 64, 64]             256\n",
      "             ReLU-48          [-1, 128, 64, 64]               0\n",
      "  ConvTranspose2d-49        [-1, 128, 128, 128]          65,664\n",
      "           Conv2d-50        [-1, 128, 128, 128]         295,040\n",
      "      BatchNorm2d-51        [-1, 128, 128, 128]             256\n",
      "             ReLU-52        [-1, 128, 128, 128]               0\n",
      "           Conv2d-53         [-1, 64, 128, 128]          73,792\n",
      "      BatchNorm2d-54         [-1, 64, 128, 128]             128\n",
      "             ReLU-55         [-1, 64, 128, 128]               0\n",
      "  ConvTranspose2d-56         [-1, 64, 256, 256]          16,448\n",
      "           Conv2d-57         [-1, 64, 256, 256]          73,792\n",
      "      BatchNorm2d-58         [-1, 64, 256, 256]             128\n",
      "             ReLU-59         [-1, 64, 256, 256]               0\n",
      "           Conv2d-60         [-1, 64, 256, 256]          36,928\n",
      "      BatchNorm2d-61         [-1, 64, 256, 256]             128\n",
      "             ReLU-62         [-1, 64, 256, 256]               0\n",
      "           Conv2d-63          [-1, 3, 256, 256]             195\n",
      "================================================================\n",
      "Total params: 23,381,251\n",
      "Trainable params: 23,381,251\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 763.50\n",
      "Params size (MB): 89.19\n",
      "Estimated Total Size (MB): 853.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = UNet()\n",
    "summary(model.cuda(),input_size=(3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9a57a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "505.382px",
    "left": "253.236px",
    "right": "20px",
    "top": "260.924px",
    "width": "783.056px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
