{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43308f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from os.path import isfile\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import transforms\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio\n",
    "from matplotlib import pyplot\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c8add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#전역변수 설정\n",
    "\n",
    "url = \"https://www.cs.toronto.edu/~vmnih/data/\"\n",
    "\n",
    "train_input_url = \"mass_roads/train/sat/index.html\"\n",
    "train_target_url = \"mass_roads/train/map/index.html\"\n",
    "val_input_url = \"mass_roads/valid/sat/index.html\"\n",
    "val_target_url = \"mass_roads/valid/map/index.html\"\n",
    "test_input_url = \"mass_roads/test/sat/index.html\"\n",
    "test_target_url = \"mass_roads/test/map/index.html\"\n",
    "\n",
    "train_directory = \"C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/train/input\"\n",
    "target_directory = \"C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/train/target\"\n",
    "val_train_directory = \"C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/validation/input\"\n",
    "val_target_directory = \"C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/validation/target\"\n",
    "test_train_directory = \"C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/test/input_images\"\n",
    "test_target_directory = \"C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/test/target_maps\"\n",
    "\n",
    "dir_train = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/train/input_filtered'\n",
    "dir_target = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/train/target_filtered'\n",
    "val_train = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/validation/input_filtered'\n",
    "val_target = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/validation/target_filtered'\n",
    "test_train = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/test/input_images_filtered'\n",
    "test_target = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/datasets/test/target_maps_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261c3360",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DataSet 설정 및 Trasform 과정\n",
    "#train 파일(R,G,B)을 img1에 저장 mask(binary)를 img2에 저장 함\n",
    "#두 파일을 합쳐서 4차원으로 stacking 하고 img3로 지정, normalization 및 crop 진행\n",
    "#crop 진행 후, img3를 다시 분할해서 img1과 img2로 나눔\n",
    "\n",
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, train_paths, target_paths, transform=None): \n",
    "        self.train_paths = [f for f in os.listdir(train_paths) if isfile(train_paths + '/'+ f)] \n",
    "        self.target_paths = [f for f in os.listdir(target_paths) if isfile(target_paths + '/' + f)]\n",
    "        self.a = train_paths\n",
    "        self.b = target_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index): \n",
    "        train = np.array(Image.open(self.a+'/'+self.train_paths[index]))\n",
    "        target = np.array(Image.open(self.b+'/'+self.target_paths[index]))\n",
    "        \n",
    "        if self.transform:\n",
    "            #transforms.Compose\n",
    "            data = self.transform(image=train, mask=target)\n",
    "            data_img = data[\"image\"].float()\n",
    "            data_lab = data[\"mask\"].float()\n",
    "            data_lab = data_lab.view([1,256,256])/255\n",
    "            \n",
    "            data = {'train': data_img, 'target': data_lab}\n",
    "        return data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_paths)\n",
    "    \n",
    "trans = A.Compose([\n",
    "    A.RandomResizedCrop(256,256),\n",
    "    A.HorizontalFlip(),\n",
    "    A.VerticalFlip(),\n",
    "    A.Normalize(mean=0.5, std=0.5),\n",
    "    transforms.ToTensorV2(transpose_mask=True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b632de",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Unet 모델 쌓기\n",
    "\n",
    "class UNet(nn.Module):  \n",
    "    def __init__(self): \n",
    "# super(subclass, self) : subclass에서 base class의 내용을 오버라이드해서 사용하고 싶을 때\n",
    "        super(UNet, self).__init__() \n",
    "        \n",
    "# 네트워크에서 반복적으로 사용되는 Conv + BatchNorm + Relu를 합쳐서 하나의 함수로 정의\n",
    "       \tdef CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers) # *으로 list unpacking \n",
    "\n",
    "            return cbr\n",
    "\n",
    "        # Contracting path\n",
    "        self.enc1_1 = CBR2d(in_channels=3, out_channels=64)\n",
    "        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n",
    "        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n",
    "        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n",
    "        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n",
    "        # Expansive path\n",
    "        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n",
    "\n",
    "        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec4_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n",
    "        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n",
    "        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec3_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n",
    "        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n",
    "        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec2_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n",
    "        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n",
    "        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec1_2 = CBR2d(in_channels=2 * 64, out_channels=64)\n",
    "        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n",
    "        self.fc = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        \n",
    "# __init__ 함수에서 선언한 layer들 연결해서 data propa flow 만들기\n",
    "    def forward(self, x):\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "        \n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "        \n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "        \n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "        \n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        dec5_1 = self.dec5_1(enc5_1)\n",
    "        \n",
    "        unpool4 = self.unpool4(dec5_1)\n",
    "        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n",
    "        dec4_2 = self.dec4_2(cat4)\n",
    "        dec4_1 = self.dec4_1(dec4_2)\n",
    "        \n",
    "        unpool3 = self.unpool3(dec4_1)\n",
    "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
    "        dec3_2 = self.dec3_2(cat3)\n",
    "        dec3_1 = self.dec3_1(dec3_2)\n",
    "\n",
    "        unpool2 = self.unpool2(dec3_1)\n",
    "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
    "        dec2_2 = self.dec2_2(cat2)\n",
    "        dec2_1 = self.dec2_1(dec2_2)\n",
    "\n",
    "        unpool1 = self.unpool1(dec2_1)\n",
    "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
    "        #tensor 합치기: Unet에서 Copy and Crop 과정을 담당한다\n",
    "        dec1_2 = self.dec1_2(cat1)\n",
    "        dec1_1 = self.dec1_1(dec1_2)\n",
    "\n",
    "        out = self.fc(dec1_1)\n",
    "\n",
    "        return out # data가 모든 layer를 거쳐서 나온 output 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7179399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#트레이닝\n",
    "## 하이퍼 파라미터 설정\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 8\n",
    "num_epoch = 100\n",
    "\n",
    "ckpt_dir = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/checkpntlog/checkpoint'\n",
    "log_dir = 'C:/Users/spinsi/Desktop/code/jupyternotebook_python37/2.Roadextraction/checkpntlog/log'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# transform 적용해서 데이터 셋 불러오기\n",
    "dataset_train = CustomDataset(dir_train, dir_target, transform=trans)\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# val set도 동일하게 진행\n",
    "dataset_val = CustomDataset(val_train, val_target, transform = trans)\n",
    "loader_val = DataLoader(dataset_val, batch_size=batch_size , shuffle=True)\n",
    "\n",
    "# 네트워크 불러오기\n",
    "net = UNet().to(device) # device : cpu or gpu\n",
    "\n",
    "# loss 정의\n",
    "fn_loss = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "# Optimizer 정의\n",
    "optim = torch.optim.Adam(net.parameters(), lr = lr ) \n",
    "\n",
    "# 기타 variables 설정\n",
    "num_train = len(dataset_train)\n",
    "num_val = len(dataset_val)\n",
    "\n",
    "\n",
    "num_train_for_epoch = np.ceil(num_train/batch_size) # np.ceil : 소수점 반올림\n",
    "num_val_for_epoch = np.ceil(num_val/batch_size)\n",
    "\n",
    "\n",
    "# 기타 function 설정\n",
    "fn_tonumpy = lambda x : x.to('cpu').detach().numpy().transpose(0,2,3,1) # device 위에 올라간 텐서를 detach 한 뒤 numpy로 변환\n",
    "fn_denorm = lambda x, mean, std : (x * std) + mean \n",
    "fn_classifier = lambda x :  1.0 * (x > 0.5)  # threshold 0.5 기준으로 indicator function으로 classifier 구현\n",
    "\n",
    "# Tensorbord\n",
    "writer_train = SummaryWriter(log_dir=os.path.join(log_dir,'train'))\n",
    "writer_val = SummaryWriter(log_dir = os.path.join(log_dir,'val'))\n",
    "writer_overall = SummaryWriter(log_dir = os.path.join(log_dir,'overall'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92f2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네트워크 저장하기\n",
    "# train을 마친 네트워크 저장 \n",
    "# net : 네트워크 파라미터, optim  두개를 dict 형태로 저장\n",
    "def save(ckpt_dir,net,optim,epoch):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    torch.save({'net':net.state_dict(),'optim':optim.state_dict()},'%s/model_epoch%d.pth'%(ckpt_dir,epoch))\n",
    "\n",
    "# 네트워크 불러오기\n",
    "def load(ckpt_dir,net,optim):\n",
    "    if not os.path.exists(ckpt_dir): # 저장된 네트워크가 없다면 인풋을 그대로 반환\n",
    "        epoch = 0\n",
    "        return net, optim, epoch\n",
    "    \n",
    "    ckpt_lst = os.listdir(ckpt_dir) # ckpt_dir 아래 있는 모든 파일 리스트를 받아온다\n",
    "    ckpt_lst.sort(key = lambda f : int(''.join(filter(str.isdigit,f))))\n",
    "\n",
    "    dict_model = torch.load('%s/%s' % (ckpt_dir,ckpt_lst[-1]))\n",
    "    net.load_state_dict(dict_model['net'])\n",
    "    optim.load_state_dict(dict_model['optim'])\n",
    "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
    "\n",
    "    return net,optim,epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcff3af",
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bf1776b96b466cab8ce5afad6d6b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid : epoch 0001 / 0100 | Batch 0001 \\ 0002 | Loss 0.245964\n",
      "valid : epoch 0001 / 0100 | Batch 0002 \\ 0002 | Loss 0.241043\n",
      "valid : epoch 0002 / 0100 | Batch 0001 \\ 0002 | Loss 0.173935\n",
      "valid : epoch 0002 / 0100 | Batch 0002 \\ 0002 | Loss 0.200217\n",
      "valid : epoch 0003 / 0100 | Batch 0001 \\ 0002 | Loss 0.160989\n",
      "valid : epoch 0003 / 0100 | Batch 0002 \\ 0002 | Loss 0.196467\n",
      "valid : epoch 0004 / 0100 | Batch 0001 \\ 0002 | Loss 0.184306\n",
      "valid : epoch 0004 / 0100 | Batch 0002 \\ 0002 | Loss 0.165440\n",
      "valid : epoch 0005 / 0100 | Batch 0001 \\ 0002 | Loss 0.194291\n",
      "valid : epoch 0005 / 0100 | Batch 0002 \\ 0002 | Loss 0.141803\n",
      "valid : epoch 0006 / 0100 | Batch 0001 \\ 0002 | Loss 0.120479\n",
      "valid : epoch 0006 / 0100 | Batch 0002 \\ 0002 | Loss 0.149474\n",
      "valid : epoch 0007 / 0100 | Batch 0001 \\ 0002 | Loss 0.159395\n",
      "valid : epoch 0007 / 0100 | Batch 0002 \\ 0002 | Loss 0.168555\n",
      "valid : epoch 0008 / 0100 | Batch 0001 \\ 0002 | Loss 0.151750\n",
      "valid : epoch 0008 / 0100 | Batch 0002 \\ 0002 | Loss 0.154649\n",
      "valid : epoch 0009 / 0100 | Batch 0001 \\ 0002 | Loss 0.127576\n",
      "valid : epoch 0009 / 0100 | Batch 0002 \\ 0002 | Loss 0.142994\n",
      "valid : epoch 0010 / 0100 | Batch 0001 \\ 0002 | Loss 0.149718\n",
      "valid : epoch 0010 / 0100 | Batch 0002 \\ 0002 | Loss 0.139699\n",
      "valid : epoch 0011 / 0100 | Batch 0001 \\ 0002 | Loss 0.146419\n",
      "valid : epoch 0011 / 0100 | Batch 0002 \\ 0002 | Loss 0.134623\n",
      "valid : epoch 0012 / 0100 | Batch 0001 \\ 0002 | Loss 0.158009\n",
      "valid : epoch 0012 / 0100 | Batch 0002 \\ 0002 | Loss 0.130587\n",
      "valid : epoch 0013 / 0100 | Batch 0001 \\ 0002 | Loss 0.145398\n",
      "valid : epoch 0013 / 0100 | Batch 0002 \\ 0002 | Loss 0.133281\n",
      "valid : epoch 0014 / 0100 | Batch 0001 \\ 0002 | Loss 0.166025\n",
      "valid : epoch 0014 / 0100 | Batch 0002 \\ 0002 | Loss 0.132691\n",
      "valid : epoch 0015 / 0100 | Batch 0001 \\ 0002 | Loss 0.146943\n",
      "valid : epoch 0015 / 0100 | Batch 0002 \\ 0002 | Loss 0.143405\n",
      "valid : epoch 0016 / 0100 | Batch 0001 \\ 0002 | Loss 0.142992\n",
      "valid : epoch 0016 / 0100 | Batch 0002 \\ 0002 | Loss 0.135822\n",
      "valid : epoch 0017 / 0100 | Batch 0001 \\ 0002 | Loss 0.116700\n",
      "valid : epoch 0017 / 0100 | Batch 0002 \\ 0002 | Loss 0.151073\n",
      "valid : epoch 0018 / 0100 | Batch 0001 \\ 0002 | Loss 0.142547\n",
      "valid : epoch 0018 / 0100 | Batch 0002 \\ 0002 | Loss 0.137985\n",
      "valid : epoch 0019 / 0100 | Batch 0001 \\ 0002 | Loss 0.110600\n",
      "valid : epoch 0019 / 0100 | Batch 0002 \\ 0002 | Loss 0.138840\n",
      "valid : epoch 0020 / 0100 | Batch 0001 \\ 0002 | Loss 0.155124\n",
      "valid : epoch 0020 / 0100 | Batch 0002 \\ 0002 | Loss 0.139439\n",
      "valid : epoch 0021 / 0100 | Batch 0001 \\ 0002 | Loss 0.116203\n",
      "valid : epoch 0021 / 0100 | Batch 0002 \\ 0002 | Loss 0.124244\n",
      "valid : epoch 0022 / 0100 | Batch 0001 \\ 0002 | Loss 0.118205\n",
      "valid : epoch 0022 / 0100 | Batch 0002 \\ 0002 | Loss 0.124759\n",
      "valid : epoch 0023 / 0100 | Batch 0001 \\ 0002 | Loss 0.109575\n",
      "valid : epoch 0023 / 0100 | Batch 0002 \\ 0002 | Loss 0.121638\n",
      "valid : epoch 0024 / 0100 | Batch 0001 \\ 0002 | Loss 0.138998\n",
      "valid : epoch 0024 / 0100 | Batch 0002 \\ 0002 | Loss 0.131168\n",
      "valid : epoch 0025 / 0100 | Batch 0001 \\ 0002 | Loss 0.121010\n",
      "valid : epoch 0025 / 0100 | Batch 0002 \\ 0002 | Loss 0.128563\n",
      "valid : epoch 0026 / 0100 | Batch 0001 \\ 0002 | Loss 0.108379\n",
      "valid : epoch 0026 / 0100 | Batch 0002 \\ 0002 | Loss 0.137875\n",
      "valid : epoch 0027 / 0100 | Batch 0001 \\ 0002 | Loss 0.106785\n",
      "valid : epoch 0027 / 0100 | Batch 0002 \\ 0002 | Loss 0.122997\n",
      "valid : epoch 0028 / 0100 | Batch 0001 \\ 0002 | Loss 0.084609\n",
      "valid : epoch 0028 / 0100 | Batch 0002 \\ 0002 | Loss 0.131171\n",
      "valid : epoch 0029 / 0100 | Batch 0001 \\ 0002 | Loss 0.127184\n",
      "valid : epoch 0029 / 0100 | Batch 0002 \\ 0002 | Loss 0.111893\n",
      "valid : epoch 0030 / 0100 | Batch 0001 \\ 0002 | Loss 0.106633\n",
      "valid : epoch 0030 / 0100 | Batch 0002 \\ 0002 | Loss 0.132383\n",
      "valid : epoch 0031 / 0100 | Batch 0001 \\ 0002 | Loss 0.146495\n",
      "valid : epoch 0031 / 0100 | Batch 0002 \\ 0002 | Loss 0.121294\n",
      "valid : epoch 0032 / 0100 | Batch 0001 \\ 0002 | Loss 0.105566\n",
      "valid : epoch 0032 / 0100 | Batch 0002 \\ 0002 | Loss 0.120425\n",
      "valid : epoch 0033 / 0100 | Batch 0001 \\ 0002 | Loss 0.081418\n",
      "valid : epoch 0033 / 0100 | Batch 0002 \\ 0002 | Loss 0.108944\n",
      "valid : epoch 0034 / 0100 | Batch 0001 \\ 0002 | Loss 0.140341\n",
      "valid : epoch 0034 / 0100 | Batch 0002 \\ 0002 | Loss 0.112129\n",
      "valid : epoch 0035 / 0100 | Batch 0001 \\ 0002 | Loss 0.131138\n",
      "valid : epoch 0035 / 0100 | Batch 0002 \\ 0002 | Loss 0.116678\n",
      "valid : epoch 0036 / 0100 | Batch 0001 \\ 0002 | Loss 0.138756\n",
      "valid : epoch 0036 / 0100 | Batch 0002 \\ 0002 | Loss 0.112924\n",
      "valid : epoch 0037 / 0100 | Batch 0001 \\ 0002 | Loss 0.122620\n",
      "valid : epoch 0037 / 0100 | Batch 0002 \\ 0002 | Loss 0.118841\n",
      "valid : epoch 0038 / 0100 | Batch 0001 \\ 0002 | Loss 0.126560\n",
      "valid : epoch 0038 / 0100 | Batch 0002 \\ 0002 | Loss 0.113096\n",
      "valid : epoch 0039 / 0100 | Batch 0001 \\ 0002 | Loss 0.116404\n",
      "valid : epoch 0039 / 0100 | Batch 0002 \\ 0002 | Loss 0.132631\n",
      "valid : epoch 0040 / 0100 | Batch 0001 \\ 0002 | Loss 0.132037\n",
      "valid : epoch 0040 / 0100 | Batch 0002 \\ 0002 | Loss 0.123720\n",
      "valid : epoch 0041 / 0100 | Batch 0001 \\ 0002 | Loss 0.147051\n",
      "valid : epoch 0041 / 0100 | Batch 0002 \\ 0002 | Loss 0.119074\n",
      "valid : epoch 0042 / 0100 | Batch 0001 \\ 0002 | Loss 0.111483\n",
      "valid : epoch 0042 / 0100 | Batch 0002 \\ 0002 | Loss 0.122880\n",
      "valid : epoch 0043 / 0100 | Batch 0001 \\ 0002 | Loss 0.123927\n",
      "valid : epoch 0043 / 0100 | Batch 0002 \\ 0002 | Loss 0.109118\n",
      "valid : epoch 0044 / 0100 | Batch 0001 \\ 0002 | Loss 0.110972\n",
      "valid : epoch 0044 / 0100 | Batch 0002 \\ 0002 | Loss 0.126819\n",
      "valid : epoch 0045 / 0100 | Batch 0001 \\ 0002 | Loss 0.148585\n",
      "valid : epoch 0045 / 0100 | Batch 0002 \\ 0002 | Loss 0.106651\n",
      "valid : epoch 0046 / 0100 | Batch 0001 \\ 0002 | Loss 0.135968\n",
      "valid : epoch 0046 / 0100 | Batch 0002 \\ 0002 | Loss 0.114305\n",
      "valid : epoch 0047 / 0100 | Batch 0001 \\ 0002 | Loss 0.139063\n",
      "valid : epoch 0047 / 0100 | Batch 0002 \\ 0002 | Loss 0.123993\n"
     ]
    }
   ],
   "source": [
    "# 네트워크 학습시키기\n",
    "start_epoch = 0\n",
    "net, optim, start_epoch = load(ckpt_dir = ckpt_dir, net = net, optim = optim) # 저장된 네트워크 불러오기\n",
    "\n",
    "for epoch in tqdm(range(start_epoch+1,num_epoch +1)):\n",
    "    net.train()\n",
    "    loss_arr = []\n",
    "    for batch, data in enumerate(loader_train,1): # 1은 뭐니 > index start point\n",
    "        # forward\n",
    "        label = data['target'].to(device)   # 데이터 device로 올리기     \n",
    "        inputs = data['train'].to(device)\n",
    "        output = net(inputs) \n",
    "\n",
    "        # backward\n",
    "        optim.zero_grad()  # gradient 초기화\n",
    "        loss = fn_loss(output, label)  # output과 label 사이의 loss 계산\n",
    "        loss.backward() # gradient backpropagation\n",
    "        optim.step() # backpropa 된 gradient를 이용해서 각 layer의 parameters update\n",
    "\n",
    "        # save loss\n",
    "        loss_arr += [loss.item()]\n",
    "\n",
    "        # tensorbord에 결과값들 저정하기\n",
    "        label = fn_tonumpy(label)\n",
    "        inputs = fn_tonumpy(fn_denorm(inputs,0.5,0.5))\n",
    "        output = fn_tonumpy(fn_classifier(output))\n",
    "\n",
    "        writer_train.add_image('label', label, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_train.add_image('input', inputs, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_train.add_image('output', output, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_overall.add_image('label_train', label, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_overall.add_image('input_train', inputs, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_overall.add_image('output_train', output, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "    \n",
    "    writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n",
    "    writer_overall.add_scalar('loss_train', np.mean(loss_arr), epoch)\n",
    "    \n",
    "    # validation\n",
    "    with torch.no_grad(): # validation 이기 때문에 backpropa 진행 x, 학습된 네트워크가 정답과 얼마나 가까운지 loss만 계산\n",
    "        net.eval() # 네트워크를 evaluation 용으로 선언\n",
    "        loss_arr = []\n",
    "\n",
    "        for batch, data in enumerate(loader_val,1):\n",
    "            # forward\n",
    "            label = data['target'].to(device)\n",
    "            inputs = data['train'].to(device)\n",
    "            output = net(inputs)\n",
    "\n",
    "            # loss \n",
    "            loss = fn_loss(output,label)\n",
    "            loss_arr += [loss.item()]\n",
    "            print('valid : epoch %04d / %04d | Batch %04d \\ %04d | Loss %04f'%(epoch,num_epoch,batch,num_val_for_epoch,np.mean(loss_arr)))\n",
    "\n",
    "            # Tensorboard 저장하기\n",
    "            label = fn_tonumpy(label)\n",
    "            inputs = fn_tonumpy(fn_denorm(inputs, mean=0.5, std=0.5))\n",
    "            output = fn_tonumpy(fn_classifier(output))\n",
    "\n",
    "            writer_val.add_image('label', label, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            writer_val.add_image('input', inputs, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            writer_val.add_image('output', output, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            writer_overall.add_image('label_val', label, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            writer_overall.add_image('input_val', inputs, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            writer_overall.add_image('output_val', output, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            \n",
    "        writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n",
    "        writer_overall.add_scalar('loss_val', np.mean(loss_arr), epoch)\n",
    "        \n",
    "        # epoch이 끝날때 마다 네트워크 저장\n",
    "        save(ckpt_dir=ckpt_dir, net = net, optim = optim, epoch = epoch)\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "writer_train.close()\n",
    "writer_val.close()\n",
    "writer_overall.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2518eeb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a41055fb5446b5bf25851738827cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid : epoch 0100 / 0100 | Batch 0001 \\ 0002 | Loss 0.050400\n",
      "valid : epoch 0100 / 0100 | Batch 0002 \\ 0002 | Loss 0.057731\n",
      "valid : epoch 0100 / 0100 | Batch 0003 \\ 0002 | Loss 0.057828\n",
      "valid : epoch 0100 / 0100 | Batch 0004 \\ 0002 | Loss 0.065320\n",
      "valid : epoch 0100 / 0100 | Batch 0005 \\ 0002 | Loss 0.069897\n",
      "valid : epoch 0100 / 0100 | Batch 0006 \\ 0002 | Loss 0.068343\n",
      "valid : epoch 0100 / 0100 | Batch 0007 \\ 0002 | Loss 0.076026\n"
     ]
    }
   ],
   "source": [
    "dataset_test = CustomDataset(test_train, test_target, transform=trans)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "writer_test = SummaryWriter(log_dir = os.path.join(log_dir,'test'))\n",
    "num_test = len(dataset_test)\n",
    "num_test_for_epoch = np.ceil(num_test/batch_size)\n",
    "\n",
    "\n",
    "\n",
    "#학습 완료된 모델을 불러온 후, 모델에 test dataset을 적용한다\n",
    "net, optim, epoch = load(ckpt_dir = ckpt_dir, net = net, optim = optim)\n",
    "with torch.no_grad(): #test 이기 때문에 backpropa 진행 x, 학습된 네트워크가 정답과 얼마나 가까운지 loss만 계산\n",
    "    net.eval() # 네트워크를 evaluation 용으로 선언\n",
    "    loss_arr = []\n",
    "    for batch, data in tqdm(enumerate(loader_test,1)):\n",
    "        # forward\n",
    "        label = data['target'].to(device)\n",
    "        inputs = data['train'].to(device)\n",
    "        output = net(inputs)\n",
    "\n",
    "        # loss \n",
    "        loss = fn_loss(output,label)\n",
    "        loss_arr += [loss.item()]\n",
    "        print('valid : epoch %04d / %04d | Batch %04d \\ %04d | Loss %04f'%(epoch,num_epoch,batch,num_val_for_epoch,np.mean(loss_arr)))\n",
    "\n",
    "        # Tensorboard 저장하기\n",
    "        label = fn_tonumpy(label)\n",
    "        inputs = fn_tonumpy(fn_denorm(inputs, mean=0.5, std=0.5))\n",
    "        output = fn_tonumpy(fn_classifier(output))\n",
    "        writer_test.add_image('label', label, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_test.add_image('input', inputs, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_test.add_image('output', output, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')        \n",
    "        writer_test.add_scalar('loss_val', np.mean(loss_arr), epoch)\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        \n",
    "writer_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "그래서 지금 해야 되는 것은 여기에 정확도를 평가하는 지표를 찾아내야 하는 것인데,\n",
    "이것이 어떻게 가능한지에 대해서 논문을 찾아볼 필요가 있을 듯\n",
    "또한 딥러닝을 이용해서 공간정보를 어떻게 이용할 지에 대해서도 곰곰히 생각해 보아야 할 문제이다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8098a87",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 1500, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#이미지 stacking\n",
    "a = os.listdir(dir_train)\n",
    "img1 = np.array(Image.open(dir_train+'/'+a[0]))\n",
    "img2 = np.array(Image.open(dir_target+'/'+a[0]))\n",
    "\n",
    "img3 = np.dstack([img1,img2])\n",
    "print(img3[:,:,0:3].shape)\n",
    "img3[:,:,3:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b412293",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'albumentations.pytorch.transforms' has no attribute 'ToPILImage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12732/2783205560.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Tensor을 Image로 출력하는 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mimg_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimg_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'albumentations.pytorch.transforms' has no attribute 'ToPILImage'"
     ]
    }
   ],
   "source": [
    "#Tensor을 Image로 출력하는 함수\n",
    "tf = transforms.ToPILImage()\n",
    "img_t = tf(dataset.__getitem__(0)[1])\n",
    "img_t.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eed764ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor을 Image로 출력하는 함수\n",
    "tf = transforms.ToPILImage()\n",
    "img_t = tf(images1[0])\n",
    "img_t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f6b871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "a,b = dataset_train.__getitem__(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed2d6a9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "808fbb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datloader 설정\n",
    "#train은 images1에 mask는 images2에 저장\n",
    "dataset = CustomDataset(dir_train, dir_target, transform=trans)\n",
    "data = dataset.__getitem__(0)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "batch_iterator = iter(train_dataloader)\n",
    "images1, images2 = next(batch_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f5cd65a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "              ReLU-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4         [-1, 64, 256, 256]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 256, 256]             128\n",
      "              ReLU-6         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-7         [-1, 64, 128, 128]               0\n",
      "            Conv2d-8        [-1, 128, 128, 128]          73,856\n",
      "       BatchNorm2d-9        [-1, 128, 128, 128]             256\n",
      "             ReLU-10        [-1, 128, 128, 128]               0\n",
      "           Conv2d-11        [-1, 128, 128, 128]         147,584\n",
      "      BatchNorm2d-12        [-1, 128, 128, 128]             256\n",
      "             ReLU-13        [-1, 128, 128, 128]               0\n",
      "        MaxPool2d-14          [-1, 128, 64, 64]               0\n",
      "           Conv2d-15          [-1, 256, 64, 64]         295,168\n",
      "      BatchNorm2d-16          [-1, 256, 64, 64]             512\n",
      "             ReLU-17          [-1, 256, 64, 64]               0\n",
      "           Conv2d-18          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-19          [-1, 256, 64, 64]             512\n",
      "             ReLU-20          [-1, 256, 64, 64]               0\n",
      "        MaxPool2d-21          [-1, 256, 32, 32]               0\n",
      "           Conv2d-22          [-1, 512, 32, 32]       1,180,160\n",
      "      BatchNorm2d-23          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-24          [-1, 512, 32, 32]               0\n",
      "           Conv2d-25          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-26          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-27          [-1, 512, 32, 32]               0\n",
      "        MaxPool2d-28          [-1, 512, 16, 16]               0\n",
      "           Conv2d-29         [-1, 1024, 16, 16]       4,719,616\n",
      "      BatchNorm2d-30         [-1, 1024, 16, 16]           2,048\n",
      "             ReLU-31         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-32          [-1, 512, 16, 16]       4,719,104\n",
      "      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-34          [-1, 512, 16, 16]               0\n",
      "  ConvTranspose2d-35          [-1, 512, 32, 32]       1,049,088\n",
      "           Conv2d-36          [-1, 512, 32, 32]       4,719,104\n",
      "      BatchNorm2d-37          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-38          [-1, 512, 32, 32]               0\n",
      "           Conv2d-39          [-1, 256, 32, 32]       1,179,904\n",
      "      BatchNorm2d-40          [-1, 256, 32, 32]             512\n",
      "             ReLU-41          [-1, 256, 32, 32]               0\n",
      "  ConvTranspose2d-42          [-1, 256, 64, 64]         262,400\n",
      "           Conv2d-43          [-1, 256, 64, 64]       1,179,904\n",
      "      BatchNorm2d-44          [-1, 256, 64, 64]             512\n",
      "             ReLU-45          [-1, 256, 64, 64]               0\n",
      "           Conv2d-46          [-1, 128, 64, 64]         295,040\n",
      "      BatchNorm2d-47          [-1, 128, 64, 64]             256\n",
      "             ReLU-48          [-1, 128, 64, 64]               0\n",
      "  ConvTranspose2d-49        [-1, 128, 128, 128]          65,664\n",
      "           Conv2d-50        [-1, 128, 128, 128]         295,040\n",
      "      BatchNorm2d-51        [-1, 128, 128, 128]             256\n",
      "             ReLU-52        [-1, 128, 128, 128]               0\n",
      "           Conv2d-53         [-1, 64, 128, 128]          73,792\n",
      "      BatchNorm2d-54         [-1, 64, 128, 128]             128\n",
      "             ReLU-55         [-1, 64, 128, 128]               0\n",
      "  ConvTranspose2d-56         [-1, 64, 256, 256]          16,448\n",
      "           Conv2d-57         [-1, 64, 256, 256]          73,792\n",
      "      BatchNorm2d-58         [-1, 64, 256, 256]             128\n",
      "             ReLU-59         [-1, 64, 256, 256]               0\n",
      "           Conv2d-60         [-1, 64, 256, 256]          36,928\n",
      "      BatchNorm2d-61         [-1, 64, 256, 256]             128\n",
      "             ReLU-62         [-1, 64, 256, 256]               0\n",
      "           Conv2d-63          [-1, 1, 256, 256]              65\n",
      "================================================================\n",
      "Total params: 23,381,121\n",
      "Trainable params: 23,381,121\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 762.50\n",
      "Params size (MB): 89.19\n",
      "Estimated Total Size (MB): 852.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = UNet()\n",
    "summary(model.cuda(),input_size=(3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3db9a57a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6004/1913578735.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConvTranspose2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python37\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attribute_name)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[[[ 1,  2,  3,  4],\n",
    "                    [ 5,  6,  7,  8],\n",
    "                    [ 9, 10, 11, 12],\n",
    "                    [13, 14, 15, 16]],\n",
    "                  [[ 1,  2,  3,  4],\n",
    "                    [ 5,  6,  7,  8],\n",
    "                    [ 9, 10, 11, 12],\n",
    "                    [13, 14, 15, 16]], \n",
    "                  [[ 1,  2,  3,  4],\n",
    "                    [ 5,  6,  7,  8],\n",
    "                    [ 9, 10, 11, 12],\n",
    "                    [13, 14, 15, 16]]],\n",
    "                 [[[ 1,  2,  3,  4],\n",
    "                    [ 5,  6,  7,  8],\n",
    "                    [ 9, 10, 11, 12],\n",
    "                    [13, 14, 15, 16]],\n",
    "                 [[ 1,  2,  3,  4],\n",
    "                    [ 5,  6,  7,  8],\n",
    "                    [ 9, 10, 11, 12],\n",
    "                    [13, 14, 15, 16]],\n",
    "                 [[ 1,  2,  3,  4],\n",
    "                    [ 5,  6,  7,  8],\n",
    "                    [ 9, 10, 11, 12],\n",
    "                    [13, 14, 15, 16]]],\n",
    "                 [[[ 1,  2,  3,  4],\n",
    "                    [ 5,  6,  7,  8],\n",
    "                    [ 9, 10, 11, 12],\n",
    "                    [13, 14, 15, 16]],\n",
    "                 [[ 1,  2,  3,  4],\n",
    "                    [ 5,  6,  7,  8],\n",
    "                    [ 9, 10, 11, 12],\n",
    "                    [13, 14, 15, 16]],\n",
    "                 [[ 1,  2,  3,  4],\n",
    "                    [ 5,  6,  7,  8],\n",
    "                    [ 9, 10, 11, 12],\n",
    "                    [13, 14, 15, 16]]]]).float()\n",
    "\n",
    "b = nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=2, stride=2, padding=0, bias=True)\n",
    "torch.cat((a,a),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d431f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "505.375px",
    "left": "345.234px",
    "right": "20px",
    "top": "241.922px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
